{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import plotly_express as px\n",
    "import pandas_profiling\n",
    "\n",
    "#lines below let allow multiple results from a line of code to be shown e.g. df.head() + df.columns\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#this allows us to see all of our columns or rows in jupyter notebook\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "#filter future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#set our code up so that it doesn't display scientific notation, we want full numbers\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Had to import the training and test set independent of each other since my sad computer is only 8GB RAM, It crashed importing them both at the same time__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "train_identity = pd.read_csv('train_identity.csv')\n",
    "train_transaction = pd.read_csv('train_transaction.csv')\n",
    "# test_identity = pd.read_csv('test_identity.csv')\n",
    "# test_transaction = pd.read_csv('test_transaction.csv')\n",
    "#merge data\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "# test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 435)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_identity = pd.read_csv('test_identity.csv')\n",
    "test_transaction = pd.read_csv('test_transaction.csv')\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 434)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"ipynb_tmp\\profile_1433235958.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x29fb03e7ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the percentage of missing values per column\n",
    "def percent_missing(dataframe):\n",
    "    '''\n",
    "    Prints the percentage of missing values for each column in a dataframe\n",
    "    '''\n",
    "    # Summing the number of missing values per column and then dividing by the total\n",
    "    sumMissing = dataframe.isnull().values.sum(axis=0)\n",
    "    pctMissing = sumMissing / dataframe.shape[0]\n",
    "    \n",
    "    if sumMissing.sum() == 0:\n",
    "        print('No missing values')\n",
    "    else:\n",
    "        # Looping through and printing out each columns missing value percentage\n",
    "        print('Percent Missing Values:', '\\n')\n",
    "        for idx, col in enumerate(dataframe.columns):\n",
    "            if sumMissing[idx] > 0:\n",
    "                print('{0}: {1:.2f}%'.format(col, pctMissing[idx] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "def filter_df_corr(inp_data, corr_val):\n",
    "    '''\n",
    "    Returns an array or dataframe (based on type(inp_data) adjusted to drop \\\n",
    "        columns with high correlation to one another. Takes second arg corr_val\n",
    "        that defines the cutoff\n",
    "\n",
    "    ----------\n",
    "    inp_data : np.array, pd.DataFrame\n",
    "        Values to consider\n",
    "    corr_val : float\n",
    "        Value [0, 1] on which to base the correlation cutoff\n",
    "    '''\n",
    "# Creates Correlation Matrix\n",
    "    if isinstance(inp_data, np.ndarray):\n",
    "        inp_data = pd.DataFrame(data=inp_data)\n",
    "        array_flag = True\n",
    "    else:\n",
    "        array_flag = False\n",
    "    corr_matrix = inp_data.corr()\n",
    "\n",
    "    # Iterates through Correlation Matrix Table to find correlated columns\n",
    "    drop_cols = []\n",
    "    n_cols = len(corr_matrix.columns)\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        for k in range(i+1, n_cols):\n",
    "            val = corr_matrix.iloc[k, i]\n",
    "            col = corr_matrix.columns[i]\n",
    "            row = corr_matrix.index[k]\n",
    "            if abs(val) >= corr_val:\n",
    "                # Prints the correlated feature set and the corr val\n",
    "                print(col, \"|\", row, \"|\", round(val, 2))\n",
    "                drop_cols.append(col)\n",
    "\n",
    "    # Drops the correlated columns\n",
    "    drop_cols = set(drop_cols)\n",
    "    inp_data = inp_data.drop(columns=drop_cols)\n",
    "    # Return same type as inp\n",
    "    if array_flag:\n",
    "        return inp_data.values\n",
    "    else:\n",
    "        return inp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop columns missing more than half their data\n",
    "# for column in train:\n",
    "#     if train[column].isna().sum() > 50000:\n",
    "#         train.drop(column, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Target Encoding Categorical Features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0253078 , 0.02881854, 0.02486205, 0.01141141,        nan])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# card_means = train.groupby('card4').isFraud.mean()\n",
    "# train['card4'] = train['card4'].map(card_means)\n",
    "# train['card4'].unique()\n",
    "\n",
    "# #remove NA values\n",
    "# train.dropna(subset = ['card4'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(-1234, inplace = True)\n",
    "test.fillna(-1234, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train['isFraud']\n",
    "features = train.drop(['isFraud'], axis = 1)\n",
    "    # #Label Encoding\n",
    "for x in features.columns[features.dtypes == 'object']:\n",
    "    features[x] = features[x].factorize()[0]\n",
    "# X_train, X_test, y_train, y_test = ms.train_test_split(features, label, test_size = .2, random_state = 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.9, gamma=0,\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=9,\n",
       "              min_child_weight=1, missing=-1234, n_estimators=500, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.9, verbosity=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(n_estimators=500,max_depth=9,learning_rate=0.05,subsample=0.9, colsample_bytree=0.9, missing = -1234)\n",
    "model.fit(features, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# X_test_predictions = lm.predict(X_test)\n",
    "# print(classification_report(y_test, X_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in test.columns[test.dtypes == 'object']:\n",
    "    test[x] = test[x].factorize()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__process of breaking the test up into pieces to predict :) gotta love 8GB RAM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.iloc[0:200000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.iloc[200000:400000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test.iloc[400000:506691, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = model.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = model.predict(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.DataFrame(pred)\n",
    "p2 = pd.DataFrame(pred2)\n",
    "p3 = pd.DataFrame(pred3)\n",
    "predictions_df = pd.concat([p1, p2, p3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scored a .73 on the leaderboard with this baseline\n",
    "- TODO: tune model hyperparameters\n",
    "- drop highly correlated features\n",
    "- explore timeseries options?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
